{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e284c20-c9e7-45db-a33d-45613e835881",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mediapipe import solutions as mp_solutions\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45934f3a-4c9c-4539-9c95-058a585f520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands model\n",
    "mp_hands = mp_solutions.hands\n",
    "mp_drawing = mp_solutions.drawing_utils\n",
    "\n",
    "### DEPTH IMAGE DATA\n",
    "\n",
    "#extract timestamp from filename\n",
    "def extract_timestamp_from_filename(file_name):\n",
    "    \"\"\"Extract timestamp from file name using regex.\"\"\"\n",
    "    match = re.search(r'_(\\d+)', file_name)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "#preprocess depth images\n",
    "def preprocess_depth_image(depth_image, min_depth=10, max_depth=1000):\n",
    "    \"\"\"Preprocess depth image by applying filtering and normalization.\"\"\"\n",
    "    #normalize depth values to [0, 1]\n",
    "    depth_image_normalized = cv2.normalize(depth_image, None, alpha=0, beta=1,\n",
    "                                           norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    #remove noisy values outside the valid depth range --> huge issue with noisy data\n",
    "    depth_image_filtered = np.where((depth_image > min_depth) & (depth_image < max_depth), depth_image, 0)\n",
    "\n",
    "    #apply median filtering to smooth noise\n",
    "    depth_image_smoothed = cv2.medianBlur(depth_image_filtered.astype(np.uint8), ksize=5)\n",
    "\n",
    "    #fill missing regions using inpainting\n",
    "    mask = (depth_image_smoothed == 0).astype(np.uint8)\n",
    "    depth_image_inpainted = cv2.inpaint(depth_image_smoothed, mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "    #apply bilateral filtering to refine edges\n",
    "    depth_image_bilateral = cv2.bilateralFilter(depth_image_inpainted, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    return depth_image_bilateral\n",
    "\n",
    "def match_rgb_and_depth_files(folder_path):\n",
    "    \"\"\"\n",
    "    Match RGB and depth files based on exact timestamps.\n",
    "    Returns a dictionary with timestamp keys and {'rgb': ..., 'depth': ...} values.\n",
    "    \"\"\"\n",
    "    file_names = os.listdir(folder_path)\n",
    "    rgb_files = [f for f in file_names if f.startswith(\"rgbFrame\")]\n",
    "    depth_files = [f for f in file_names if f.startswith(\"depthFrame\")]\n",
    "\n",
    "    # Build dictionaries by timestamp\n",
    "    rgb_dict = {extract_timestamp(f): f for f in rgb_files}\n",
    "    depth_dict = {extract_timestamp(f): f for f in depth_files}\n",
    "\n",
    "    # Only keep matching pairs\n",
    "    matched_timestamps = set(rgb_dict.keys()).intersection(depth_dict.keys())\n",
    "\n",
    "    matched_frames = {\n",
    "        ts: {'rgb': rgb_dict[ts], 'depth': depth_dict[ts]}\n",
    "        for ts in matched_timestamps\n",
    "    }\n",
    "\n",
    "    print(f\" Matched {len(matched_frames)} frame pairs.\")\n",
    "    return matched_frames\n",
    "\n",
    "\n",
    "\n",
    "#validate landmarks using depth data\n",
    "def validate_landmarks(rgb_image, depth_image, hand_landmarks):\n",
    "    \"\"\"Validate hand landmarks using depth data.\"\"\"\n",
    "    h, w = depth_image.shape\n",
    "    for landmark in hand_landmarks.landmark:\n",
    "        cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "        if not (0 <= cx < w and 0 <= cy < h and depth_image[cy, cx] > 0):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#compute angles between three points\n",
    "def compute_angle(a, b, c):\n",
    "    \"\"\"Compute the angle formed by three points.\"\"\"\n",
    "    ab = np.array(b) - np.array(a)\n",
    "    bc = np.array(c) - np.array(b)\n",
    "    cos_theta = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc))\n",
    "    angle = np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "    return angle\n",
    "\n",
    "def compute_vector_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Compute the angle between vectors ab and bc using the dot product.\n",
    "    Parameters:\n",
    "        a, b, c: 3D coordinates of points (list or ndarray).\n",
    "    \"\"\"\n",
    "    ab = np.array(b) - np.array(a)\n",
    "    bc = np.array(c) - np.array(b)\n",
    "    cos_theta = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc))\n",
    "    return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "\n",
    "def compute_abduction_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two points.\n",
    "    Parameters:\n",
    "        point1, point2: 3D coordinates of points.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "def process_frames_and_compute_angles(folder_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True):\n",
    "    \"\"\"\n",
    "    Process RGB and depth frames, compute joint angles, and visualize 3D hand poses.\n",
    "    Uses functions from the referenced paper to calculate flexion and abduction angles.\n",
    "    \"\"\"\n",
    "    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "    for i, (timestamp, files) in enumerate(matched_frames.items()):\n",
    "        rgb_path = os.path.join(folder_path, files['rgb'])\n",
    "        depth_path = os.path.join(folder_path, files['depth'])\n",
    "\n",
    "        # Load RGB and depth images\n",
    "        rgb_image = cv2.imread(rgb_path)\n",
    "        depth_image = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if use_preprocessed:\n",
    "            # Apply preprocessing to depth image\n",
    "            depth_image = preprocess_depth_image(depth_image)\n",
    "\n",
    "        # Resize RGB to match depth dimensions\n",
    "        depth_height, depth_width = depth_image.shape\n",
    "        rgb_image_resized = cv2.resize(rgb_image, (depth_width, depth_height))\n",
    "        rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process with MediaPipe Hands\n",
    "        results = hands.process(rgb_image_rgb)\n",
    "        if not results.multi_hand_landmarks:\n",
    "            continue\n",
    "\n",
    "        # Process each detected hand\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            landmarks_3d = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                cx, cy = int(landmark.x * depth_width), int(landmark.y * depth_height)\n",
    "                z = depth_image[cy, cx] if (0 <= cx < depth_width and 0 <= cy < depth_height) else 0\n",
    "                landmarks_3d.append([cx, cy, z])\n",
    "\n",
    "            # Compute angles for each finger\n",
    "            for finger, indices in finger_indices.items():\n",
    "                if len(landmarks_3d) >= max(indices) + 1:\n",
    "                    # Flexion Angles\n",
    "                    mcp_angle = compute_vector_angle(landmarks_3d[indices[0]],\n",
    "                                                     landmarks_3d[indices[1]],\n",
    "                                                     landmarks_3d[indices[2]])\n",
    "                    pip_angle = compute_vector_angle(landmarks_3d[indices[1]],\n",
    "                                                     landmarks_3d[indices[2]],\n",
    "                                                     landmarks_3d[indices[3]])\n",
    "                    dip_angle = compute_vector_angle(landmarks_3d[indices[2]],\n",
    "                                                     landmarks_3d[indices[3]],\n",
    "                                                     landmarks_3d[indices[4]])\n",
    "\n",
    "                    # Abduction Distances (between adjacent fingertips)\n",
    "                    if finger != \"thumb\":  # Thumb abduction is handled separately\n",
    "                        prev_tip = landmarks_3d[finger_indices[finger][4] - 4]\n",
    "                        current_tip = landmarks_3d[indices[4]]\n",
    "                        abduction_distance = compute_abduction_distance(prev_tip, current_tip)\n",
    "\n",
    "                        print(f\"{finger.capitalize()} Finger Abduction Distance - Frame {i}: {abduction_distance:.2f} mm\")\n",
    "\n",
    "                    print(f\"{finger.capitalize()} Finger Angles - Frame {i}:\")\n",
    "                    print(f\"  MCP: {mcp_angle:.2f}Â°\")\n",
    "                    print(f\"  PIP: {pip_angle:.2f}Â°\")\n",
    "                    print(f\"  DIP: {dip_angle:.2f}Â°\")\n",
    "\n",
    "            # Visualize the 3D Hand Pose\n",
    "            visualize_3d_hand_pose_with_rgb(landmarks_3d, finger_indices, finger_colors, i)\n",
    "\n",
    "    hands.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb36f6-159d-4a06-b221-f88e0524a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "folder_path = \"Desktop/study ROM/Nelson/nelson_1\"\n",
    "finger_indices = {\n",
    "    \"thumb\": [0, 1, 2, 3, 4],\n",
    "    \"index\": [0, 5, 6, 7, 8],\n",
    "    \"middle\": [0, 9, 10, 11, 12],\n",
    "    \"ring\": [0, 13, 14, 15, 16],\n",
    "    \"pinky\": [0, 17, 18, 19, 20],\n",
    "}\n",
    "finger_colors = {\n",
    "    \"thumb\": \"red\",\n",
    "    \"index\": \"green\",\n",
    "    \"middle\": \"blue\",\n",
    "    \"ring\": \"orange\",\n",
    "    \"pinky\": \"purple\",\n",
    "}\n",
    "\n",
    "matched_frames = match_rgb_and_depth_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc529c0f-cb47-4185-9b65-100144cc8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Desktop/study ROM/Nelson/nelson_1\"\n",
    "matched_frames = match_rgb_and_depth_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b5ffa-a47d-449a-bd0f-8f17216d6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d_hand_pose_with_rgb(landmarks_3d, rgb_frame, frame_index):\n",
    "    \"\"\"\n",
    "    Visualize 3D hand pose alongside the corresponding RGB frame using matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks_3d (list): List of 3D landmarks as [x, y, z].\n",
    "        rgb_frame (numpy.ndarray): RGB frame as a numpy array (from cv2.imread or similar).\n",
    "        frame_index (int): Current frame index for visualization.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Ensure RGB frame is valid\n",
    "    if rgb_frame is None or rgb_frame.size == 0:\n",
    "        print(\"Error: RGB frame is empty or invalid.\")\n",
    "        return\n",
    "\n",
    "    # External definitions of finger_indices and finger_colors\n",
    "    global finger_indices, finger_colors\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the RGB frame\n",
    "    ax1 = fig.add_subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "    try:\n",
    "        ax1.imshow(cv2.cvtColor(rgb_frame, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for display\n",
    "        ax1.set_title(f\"RGB Frame - Frame {frame_index}\")\n",
    "        ax1.axis('off')  # Hide axis for the image\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying RGB frame: {e}\")\n",
    "        return\n",
    "\n",
    "    # Plot the 3D hand pose\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection='3d')  # 1 row, 2 columns, 2nd subplot\n",
    "    try:\n",
    "        xs = [pt[0] for pt in landmarks_3d]\n",
    "        ys = [pt[1] for pt in landmarks_3d]\n",
    "        zs = [pt[2] for pt in landmarks_3d]\n",
    "\n",
    "        # Plot all landmarks\n",
    "        ax2.scatter(xs, ys, zs, c='black', s=20, label='Landmarks')\n",
    "\n",
    "        # Connect landmarks for each finger\n",
    "        for finger, indices in finger_indices.items():\n",
    "            finger_points = [landmarks_3d[i] for i in indices]\n",
    "            x_finger = [pt[0] for pt in finger_points]\n",
    "            y_finger = [pt[1] for pt in finger_points]\n",
    "            z_finger = [pt[2] for pt in finger_points]\n",
    "\n",
    "            # Use predefined color for the finger\n",
    "            color = finger_colors[finger]\n",
    "            ax2.plot(x_finger, y_finger, z_finger, label=finger.capitalize(), color=color)\n",
    "\n",
    "        # Set axis labels and title\n",
    "        ax2.set_xlabel('X (pixels)')\n",
    "        ax2.set_ylabel('Y (pixels)')\n",
    "        ax2.set_zlabel('Z (depth)')\n",
    "        ax2.set_title(f\"3D Hand Pose - Frame {frame_index}\")\n",
    "        ax2.legend()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying 3D plot: {e}\")\n",
    "        return\n",
    "\n",
    "    # Display both subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def process_frames_and_debug_compute_angles(folder_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True):\n",
    "    \"\"\"\n",
    "    Process RGB and depth frames, debug raw depth data loading, compute joint angles,\n",
    "    and visualize 3D hand poses.\n",
    "    \"\"\"\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "    for i, (timestamp, files) in enumerate(matched_frames.items()):\n",
    "        rgb_path = os.path.join(folder_path, files['rgb'])\n",
    "        depth_path = os.path.join(folder_path, files['depth'])\n",
    "\n",
    "        print(f\"\\nProcessing Frame {i}: Timestamp {timestamp}\")\n",
    "        print(f\"RGB Path: {rgb_path}\")\n",
    "        print(f\"Depth Path: {depth_path}\")\n",
    "\n",
    "        # Load RGB and depth images\n",
    "        rgb_image = cv2.imread(rgb_path)\n",
    "        if rgb_image is None:\n",
    "            print(f\"Error: Failed to load RGB image at {rgb_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(depth_path, 'rb') as f:\n",
    "                depth_image = np.fromfile(f, dtype=np.float32)\n",
    "                print(f\"Loaded depth image of size: {depth_image.size}\")  # Should be 230400\n",
    "            \n",
    "                # Calculate dimensions\n",
    "                height = depth_image.size // 640\n",
    "                depth_image = depth_image.reshape((height, 640))\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading depth file {depth_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if use_preprocessed:\n",
    "            # Apply preprocessing to depth image\n",
    "            try:\n",
    "                depth_image = preprocess_depth_image(depth_image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error preprocessing depth image: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Resize RGB to match depth dimensions\n",
    "        depth_height, depth_width = depth_image.shape\n",
    "        try:\n",
    "            rgb_image_resized = cv2.resize(rgb_image, (depth_width, depth_height))\n",
    "            rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(f\"Error resizing or converting RGB image: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Process with MediaPipe Hands\n",
    "        results = hands.process(rgb_image_rgb)\n",
    "        if not results.multi_hand_landmarks:\n",
    "            print(\"No hands detected in this frame.\")\n",
    "            continue\n",
    "\n",
    "        # Process each detected hand\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            landmarks_3d = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                cx, cy = int(landmark.x * depth_width), int(landmark.y * depth_height)\n",
    "                z = depth_image[cy, cx] if (0 <= cx < depth_width and 0 <= cy < depth_height) else 0\n",
    "                landmarks_3d.append([cx, cy, z])\n",
    "\n",
    "            # Compute angles for each finger\n",
    "            for finger, indices in finger_indices.items():\n",
    "                if len(landmarks_3d) >= max(indices) + 1:\n",
    "                    try:\n",
    "                        # Flexion Angles\n",
    "                        mcp_angle = compute_vector_angle(landmarks_3d[indices[0]],\n",
    "                                                         landmarks_3d[indices[1]],\n",
    "                                                         landmarks_3d[indices[2]])\n",
    "                        pip_angle = compute_vector_angle(landmarks_3d[indices[1]],\n",
    "                                                         landmarks_3d[indices[2]],\n",
    "                                                         landmarks_3d[indices[3]])\n",
    "                        dip_angle = compute_vector_angle(landmarks_3d[indices[2]],\n",
    "                                                         landmarks_3d[indices[3]],\n",
    "                                                         landmarks_3d[indices[4]])\n",
    "\n",
    "                        print(f\"{finger.capitalize()} Finger Angles - Frame {i}:\")\n",
    "                        print(f\"  MCP: {mcp_angle:.2f}Â°\")\n",
    "                        print(f\"  PIP: {pip_angle:.2f}Â°\")\n",
    "                        print(f\"  DIP: {dip_angle:.2f}Â°\")\n",
    "\n",
    "                        # Abduction Distances\n",
    "                        if finger != \"thumb\":  # Thumb abduction is handled separately\n",
    "                            prev_tip = landmarks_3d[finger_indices[finger][4] - 4]\n",
    "                            current_tip = landmarks_3d[indices[4]]\n",
    "                            abduction_distance = compute_abduction_distance(prev_tip, current_tip)\n",
    "                            print(f\"{finger.capitalize()} Finger Abduction Distance - Frame {i}: {abduction_distance:.2f} mm\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error computing angles or distances for {finger}: {e}\")\n",
    "\n",
    "            # Visualize the 3D Hand Pose\n",
    "            try:\n",
    "                visualize_3d_hand_pose_with_rgb(landmarks_3d, rgb_image_rgb, i)\n",
    "            except Exception as e:\n",
    "                print(f\"Error visualizing 3D hand pose: {e}\")\n",
    "\n",
    "    hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c2b2a-8fa2-4745-a731-c8acbb50914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_frames_and_debug_compute_angles(folder_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c4a6f-20a1-4da3-b3e7-ec82f0ba094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_and_save_video_output(folder_path, matched_frames, finger_indices, finger_colors, use_preprocessed=True, output_path='hand_pose_output.mp4'):\n",
    "    \"\"\"\n",
    "    Process frames and save a video of RGB and 3D hand pose side-by-side.\n",
    "    \"\"\"\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i, (timestamp, files) in enumerate(matched_frames.items()):\n",
    "        rgb_path = os.path.join(folder_path, files['rgb'])\n",
    "        depth_path = os.path.join(folder_path, files['depth'])\n",
    "\n",
    "        rgb_image = cv2.imread(rgb_path)\n",
    "        if rgb_image is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(depth_path, 'rb') as f:\n",
    "                depth_image = np.fromfile(f, dtype=np.float32)\n",
    "                depth_image = depth_image.reshape((depth_image.size // 640, 640))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if use_preprocessed:\n",
    "            try:\n",
    "                depth_image = preprocess_depth_image(depth_image)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        depth_height, depth_width = depth_image.shape\n",
    "        try:\n",
    "            rgb_image_resized = cv2.resize(rgb_image, (depth_width, depth_height))\n",
    "            rgb_image_rgb = cv2.cvtColor(rgb_image_resized, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        results = hands.process(rgb_image_rgb)\n",
    "        if not results.multi_hand_landmarks:\n",
    "            continue\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            landmarks_3d = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                cx, cy = int(landmark.x * depth_width), int(landmark.y * depth_height)\n",
    "                z = depth_image[cy, cx] if (0 <= cx < depth_width and 0 <= cy < depth_height) else 0\n",
    "                landmarks_3d.append([cx, cy, z])\n",
    "\n",
    "            # Create figure with side-by-side plots\n",
    "            fig = plt.figure(figsize=(12, 6))\n",
    "            ax1 = fig.add_subplot(1, 2, 1)\n",
    "            ax1.imshow(rgb_image_rgb)\n",
    "            ax1.set_title(f\"RGB Frame {i}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "            xs = [pt[0] for pt in landmarks_3d]\n",
    "            ys = [pt[1] for pt in landmarks_3d]\n",
    "            zs = [pt[2] for pt in landmarks_3d]\n",
    "            ax2.scatter(xs, ys, zs, c='black', s=20)\n",
    "\n",
    "            for finger, indices in finger_indices.items():\n",
    "                finger_points = [landmarks_3d[i] for i in indices]\n",
    "                x_f = [p[0] for p in finger_points]\n",
    "                y_f = [p[1] for p in finger_points]\n",
    "                z_f = [p[2] for p in finger_points]\n",
    "                ax2.plot(x_f, y_f, z_f, color=finger_colors[finger], label=finger)\n",
    "\n",
    "            ax2.set_xlim([0, depth_width])\n",
    "            ax2.set_ylim([depth_height, 0])\n",
    "            ax2.set_zlim([0, np.max(zs)*1.2 if len(zs) > 0 else 1])\n",
    "            ax2.set_title(f\"3D Hand Pose - Frame {i}\")\n",
    "            ax2.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.canvas.draw()\n",
    "\n",
    "            # Convert figure to image array\n",
    "            img_np = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            img_np = img_np.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            frames.append(img_np)\n",
    "\n",
    "            plt.close(fig)\n",
    "\n",
    "    hands.close()\n",
    "\n",
    "    # Save video\n",
    "    if frames:\n",
    "        height, width, _ = frames[0].shape\n",
    "        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 5, (width, height))\n",
    "        for frame in frames:\n",
    "            bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            out.write(bgr)\n",
    "        out.release()\n",
    "        print(f\" Video saved to {output_path}\")\n",
    "    else:\n",
    "        print(\" No frames were processed. Video not saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37041341-4ea2-4a01-b11b-72dbbd3fc4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_rgb_video_from_frames(folder_path, output_filename=\"rgb_video_output.mp4\", fps=30):\n",
    "    \"\"\"\n",
    "    Creates a 30 FPS video from RGB frames using their original resolution.\n",
    "    \"\"\"\n",
    "    file_names = sorted([f for f in os.listdir(folder_path) if f.startswith('rgbFrame') and f.endswith('.png')])\n",
    "\n",
    "    if not file_names:\n",
    "        print(\" No RGB frames found.\")\n",
    "        return\n",
    "\n",
    "    output_path = os.path.join(os.path.dirname(folder_path), output_filename)\n",
    "\n",
    "    # Get frame size from the first image\n",
    "    first_frame = cv2.imread(os.path.join(folder_path, file_names[0]))\n",
    "    if first_frame is None:\n",
    "        print(\" Could not read the first frame.\")\n",
    "        return\n",
    "\n",
    "    height, width, _ = first_frame.shape\n",
    "    video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    for filename in file_names:\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        frame = cv2.imread(path)\n",
    "        if frame is not None:\n",
    "            video_writer.write(frame)\n",
    "        else:\n",
    "            print(f\" Skipped: {filename}\")\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\" 30 FPS RGB video saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a8756-e749-46f8-a99f-1ba00074201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Desktop/study ROM/Andrew/2\"\n",
    "create_rgb_video_from_frames(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9b2b8-a26c-45fb-8954-1446acf4c27c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4def491-f64e-42b8-b5bb-49c40919f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATA_PATH = \"Desktop/Classification_ROM\"  \n",
    "\n",
    "CLASSES = {'healthy': 0, 'limited': 1}\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    class_dir = os.path.join(DATA_PATH, class_name)\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(class_dir)\n",
    "        if os.path.isdir(os.path.join(class_dir, d)) and not d.startswith('.')\n",
    "    ])\n",
    "    print(f\"\\nClass: {class_name}\")\n",
    "    print(\"Subjects found:\", subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d472e-984b-40be-a48b-486d94dad1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_number(file_name):\n",
    "    match = re.search(r'_(\\d+)', file_name)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "subject_path = os.path.join(DATA_PATH, 'healthy', 'S1')\n",
    "all_files = sorted(os.listdir(subject_path))\n",
    "\n",
    "rgb_files = [f for f in all_files if f.startswith('rgbFrame')]\n",
    "depth_files = [f for f in all_files if f.startswith('rawDepth')]\n",
    "\n",
    "frame_ids = sorted(set(extract_number(f) for f in rgb_files if extract_number(f) is not None))\n",
    "\n",
    "print(\"Example RGB files:\", rgb_files[:5])\n",
    "print(\"Example Depth files:\", depth_files[:5])\n",
    "print(\"Extracted frame numbers:\", frame_ids[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765f552-e88b-4771-adb4-da554c2f01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamp(filename):\n",
    "    match = re.search(r'_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "subject_path = os.path.join(DATA_PATH, 'healthy', 'S1')\n",
    "all_files = sorted(os.listdir(subject_path))\n",
    "\n",
    "rgb_files = [f for f in all_files if f.startswith('rgbFrame')]\n",
    "depth_files = [f for f in all_files if f.startswith('rawDepth')]\n",
    "\n",
    "# Match files by timestamp\n",
    "timestamps_rgb = {extract_timestamp(f): f for f in rgb_files}\n",
    "timestamps_depth = {extract_timestamp(f): f for f in depth_files}\n",
    "\n",
    "# Find common timestamps\n",
    "matched_timestamps = sorted(set(timestamps_rgb.keys()) & set(timestamps_depth.keys()))\n",
    "\n",
    "if matched_timestamps:\n",
    "    print(\"Matched timestamps:\", matched_timestamps[:10])\n",
    "    print(\"Example matched pair:\",\n",
    "          timestamps_rgb[matched_timestamps[0]],\n",
    "          timestamps_depth[matched_timestamps[0]])\n",
    "else:\n",
    "    print(\" No matched depth files found for healthy/S1.\")\n",
    "    print(\"You can proceed with RGB-only model first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322beddd-aa1f-4b18-8cd8-e4294e9fa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WINDOW_SIZE = 10\n",
    "X_rgb = []\n",
    "y = []\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    match = re.search(r'_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_rgb_sequence_features(image_paths):\n",
    "    coords = []\n",
    "\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=1) as hands:\n",
    "        for path in image_paths:\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(img_rgb)\n",
    "\n",
    "            if not result.multi_hand_landmarks:\n",
    "                continue\n",
    "\n",
    "            hand = result.multi_hand_landmarks[0]\n",
    "            coords.append(np.array([(lm.x, lm.y) for lm in hand.landmark]).flatten())\n",
    "\n",
    "    if len(coords) < WINDOW_SIZE:\n",
    "        return None\n",
    "\n",
    "    coords = np.array(coords[:WINDOW_SIZE])\n",
    "    return np.concatenate([np.mean(coords, axis=0), np.std(coords, axis=0)])\n",
    "\n",
    "# Loop through all classes and subjects\n",
    "for class_name, label in CLASSES.items():\n",
    "    class_dir = os.path.join(DATA_PATH, class_name)\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(class_dir)\n",
    "        if os.path.isdir(os.path.join(class_dir, d)) and not d.startswith('.')\n",
    "    ])\n",
    "\n",
    "    for subj in subjects:\n",
    "        subj_path = os.path.join(class_dir, subj)\n",
    "        files = sorted(os.listdir(subj_path))\n",
    "        rgb_files = [f for f in files if f.startswith('rgbFrame')]\n",
    "        timestamps = sorted([extract_timestamp(f) for f in rgb_files if extract_timestamp(f)])\n",
    "\n",
    "        for i in range(0, len(timestamps) - WINDOW_SIZE + 1, WINDOW_SIZE):\n",
    "            window_ts = timestamps[i:i + WINDOW_SIZE]\n",
    "            rgb_paths = [os.path.join(subj_path, f\"rgbFrame_{ts}.png\") for ts in window_ts]\n",
    "            feat = extract_rgb_sequence_features(rgb_paths)\n",
    "            if feat is not None:\n",
    "                X_rgb.append(feat)\n",
    "                y.append(label)\n",
    "\n",
    "print(f\"Extracted {len(X_rgb)} RGB-only sequences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9dee7f-bd09-4bc5-ae62-fa003baa3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_rgb, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Report\n",
    "print(\"\\nðŸŽ¯ Classification Report (RGB only):\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Healthy', 'Limited']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Limited'], yticklabels=['Healthy', 'Limited'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (RGB Only)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f4ae4-60e4-4806-a3ee-a847f991fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "WINDOW_SIZE = 10\n",
    "SEQ_X = []\n",
    "SEQ_Y = []\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    match = re.search(r'_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_raw_sequence(image_paths):\n",
    "    coords = []\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=1) as hands:\n",
    "        for path in image_paths:\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(img_rgb)\n",
    "            if not result.multi_hand_landmarks:\n",
    "                return None  # skip sequences with missing hands\n",
    "            hand = result.multi_hand_landmarks[0]\n",
    "            coords.append(np.array([(lm.x, lm.y) for lm in hand.landmark]).flatten())\n",
    "    if len(coords) < WINDOW_SIZE:\n",
    "        return None\n",
    "    return np.array(coords[:WINDOW_SIZE])  # shape: (10, 42)\n",
    "\n",
    "# Main loop\n",
    "SEQ_X = []\n",
    "SEQ_Y = []\n",
    "\n",
    "for class_name, label in CLASSES.items():\n",
    "    class_dir = os.path.join(DATA_PATH, class_name)\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(class_dir)\n",
    "        if os.path.isdir(os.path.join(class_dir, d)) and not d.startswith('.')\n",
    "    ])\n",
    "    for subj in subjects:\n",
    "        subj_path = os.path.join(class_dir, subj)\n",
    "        files = sorted(os.listdir(subj_path))\n",
    "        rgb_files = [f for f in files if f.startswith('rgbFrame')]\n",
    "        timestamps = sorted([extract_timestamp(f) for f in rgb_files if extract_timestamp(f)])\n",
    "\n",
    "        for i in range(0, len(timestamps) - WINDOW_SIZE + 1, WINDOW_SIZE):\n",
    "            window_ts = timestamps[i:i + WINDOW_SIZE]\n",
    "            rgb_paths = [os.path.join(subj_path, f\"rgbFrame_{ts}.png\") for ts in window_ts]\n",
    "            sequence = extract_raw_sequence(rgb_paths)\n",
    "            if sequence is not None:\n",
    "                SEQ_X.append(sequence)  # (10, 42)\n",
    "                SEQ_Y.append(label)\n",
    "\n",
    "SEQ_X = np.array(SEQ_X)  # shape: (num_sequences, 10, 42)\n",
    "SEQ_Y = np.array(SEQ_Y)\n",
    "\n",
    "print(\" Shape of sequence data:\", SEQ_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b070a4-28f1-4b2b-9dc8-7aa12d3daec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reshape to apply PCA: (3420, 42)\n",
    "X_flat = SEQ_X.reshape(-1, 42)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=10)  # Choose based on explained variance or course alignment\n",
    "X_pca_flat = pca.fit_transform(X_flat)\n",
    "\n",
    "# Reshape back to sequences: (342, 10, 10)\n",
    "X_seq_pca = X_pca_flat.reshape(SEQ_X.shape[0], SEQ_X.shape[1], -1)\n",
    "\n",
    "print(\" After PCA, shape is:\", X_seq_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0938bd-0d38-478e-817a-9bd42a80645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb72137-283e-4ec9-a836-20c8fca8eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reshape to apply PCA: (3420, 42)\n",
    "X_flat = SEQ_X.reshape(-1, 42)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=10)  # Choose based on explained variance or course alignment\n",
    "X_pca_flat = pca.fit_transform(X_flat)\n",
    "\n",
    "# Reshape back to sequences: (342, 10, 10)\n",
    "X_seq_pca = X_pca_flat.reshape(SEQ_X.shape[0], SEQ_X.shape[1], -1)\n",
    "\n",
    "print(\" After PCA, shape is:\", X_seq_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04222c-bfc1-4e16-aba5-5f3f41cb4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq_pca, SEQ_Y, test_size=0.2, stratify=SEQ_Y, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Define model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 10)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215d55c-3691-4899-9153-40294dd539aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Report\n",
    "print(\"\\nðŸ“Š LSTM Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Healthy', 'Limited']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d18bb9-cc29-43d7-848e-7148ccb19e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "# Flatten all frames: shape (3420, 42)\n",
    "X_flat = SEQ_X.reshape(-1, 42)\n",
    "\n",
    "# Apply Random Projection\n",
    "rp = GaussianRandomProjection(n_components=10, random_state=42)\n",
    "X_rp_flat = rp.fit_transform(X_flat)\n",
    "\n",
    "# Reshape back: (342, 10, 10)\n",
    "X_seq_rp = X_rp_flat.reshape(SEQ_X.shape[0], SEQ_X.shape[1], -1)\n",
    "\n",
    "print(\" After Random Projection, shape is:\", X_seq_rp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526cb18-2ad4-4f9f-a312-57492a964e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train_rp, X_test_rp, y_train_rp, y_test_rp = train_test_split(\n",
    "    X_seq_rp, SEQ_Y, test_size=0.2, stratify=SEQ_Y, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_rp_cat = to_categorical(y_train_rp)\n",
    "y_test_rp_cat = to_categorical(y_test_rp)\n",
    "\n",
    "# Define LSTM model\n",
    "model_rp = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 10)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_rp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train with early stopping\n",
    "history_rp = model_rp.fit(\n",
    "    X_train_rp, y_train_rp_cat,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb83e04-c3f0-49b8-a898-4dcee6109d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_rp = model_rp.predict(X_test_rp)\n",
    "y_pred_rp_class = np.argmax(y_pred_rp, axis=1)\n",
    "y_true_rp = np.argmax(y_test_rp_cat, axis=1)\n",
    "\n",
    "print(\"\\n LSTM with Random Projection:\\n\")\n",
    "print(classification_report(y_true_rp, y_pred_rp_class, target_names=[\"Healthy\", \"Limited\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e5ad7-6c87-44ca-961c-3c834fb615a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_depth_raw(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        depth = np.fromfile(f, dtype=np.float32)\n",
    "    return depth.reshape((480, 640))  # Confirm this matches your image resolution\n",
    "\n",
    "def extract_3d_sequence(rgb_paths, depth_paths):\n",
    "    coords = []\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=1) as hands:\n",
    "        for rgb_path, depth_path in zip(rgb_paths, depth_paths):\n",
    "            img = cv2.imread(rgb_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(img_rgb)\n",
    "            if not result.multi_hand_landmarks:\n",
    "                return None\n",
    "            hand = result.multi_hand_landmarks[0]\n",
    "\n",
    "            # Load depth\n",
    "            depth_img = load_depth_raw(depth_path)\n",
    "            h, w = img_rgb.shape[:2]\n",
    "\n",
    "            # 3D landmarks\n",
    "            coords_3d = []\n",
    "            for lm in hand.landmark:\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                z = depth_img[cy, cx] if (0 <= cx < 640 and 0 <= cy < 480) else 0\n",
    "                coords_3d.append([lm.x, lm.y, z])\n",
    "            coords.append(np.array(coords_3d).flatten())\n",
    "\n",
    "    if len(coords) < WINDOW_SIZE:\n",
    "        return None\n",
    "    return np.array(coords[:WINDOW_SIZE])  # shape: (10, 63)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabebcc4-2775-4331-a582-c49ec851d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "mp_hands = mp.solutions.hands\n",
    "WINDOW_SIZE = 10\n",
    "DATA_PATH = \"Desktop/Classification_ROM\"  # <-- update this\n",
    "CLASSES = {'healthy': 0, 'limited': 1}\n",
    "\n",
    "# Extract timestamp from filenames like rgbFrame_1745435851837.png\n",
    "def extract_timestamp(filename):\n",
    "    match = re.search(r'_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Load depth from .raw file\n",
    "def load_depth_raw(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        depth = np.fromfile(f, dtype=np.float32)\n",
    "    return depth.reshape((360, 640))  # Adjust if your depth resolution is different\n",
    "\n",
    "# Extract 3D (x, y, z) sequence from a 10-frame window\n",
    "def extract_3d_sequence(rgb_paths, depth_paths):\n",
    "    coords = []\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=1) as hands:\n",
    "        for rgb_path, depth_path in zip(rgb_paths, depth_paths):\n",
    "            img = cv2.imread(rgb_path)\n",
    "            if img is None:\n",
    "                return None\n",
    "\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(img_rgb)\n",
    "            if not result.multi_hand_landmarks:\n",
    "                return None\n",
    "\n",
    "            hand = result.multi_hand_landmarks[0]\n",
    "            depth_img = load_depth_raw(depth_path)\n",
    "            h, w = img_rgb.shape[:2]\n",
    "\n",
    "            coords_3d = []\n",
    "            for lm in hand.landmark:\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                z = depth_img[cy, cx] if (0 <= cx < 640 and 0 <= cy < 480) else 0\n",
    "                coords_3d.append([lm.x, lm.y, z])\n",
    "\n",
    "            coords.append(np.array(coords_3d).flatten())\n",
    "\n",
    "    if len(coords) < WINDOW_SIZE:\n",
    "        return None\n",
    "    return np.array(coords[:WINDOW_SIZE])  # Shape: (10, 63)\n",
    "\n",
    "# MAIN: Loop over data and extract RGBD 3D sequences\n",
    "SEQ_X_3D = []\n",
    "SEQ_Y_3D = []\n",
    "\n",
    "for class_name, label in CLASSES.items():\n",
    "    class_dir = os.path.join(DATA_PATH, class_name)\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(class_dir)\n",
    "        if os.path.isdir(os.path.join(class_dir, d)) and not d.startswith('.')\n",
    "    ])\n",
    "    for subj in subjects:\n",
    "        subj_path = os.path.join(class_dir, subj)\n",
    "        files = sorted(os.listdir(subj_path))\n",
    "        rgb_files = [f for f in files if f.startswith('rgbFrame')]\n",
    "        depth_files = [f for f in files if f.startswith('depthFrame')]\n",
    "\n",
    "        timestamps_rgb = {extract_timestamp(f): f for f in rgb_files}\n",
    "        timestamps_depth = {extract_timestamp(f): f for f in depth_files}\n",
    "        matched_ts = sorted(set(timestamps_rgb) & set(timestamps_depth))\n",
    "\n",
    "        for i in range(0, len(matched_ts) - WINDOW_SIZE + 1, WINDOW_SIZE):\n",
    "            window_ts = matched_ts[i:i + WINDOW_SIZE]\n",
    "            rgb_paths = [os.path.join(subj_path, timestamps_rgb[ts]) for ts in window_ts]\n",
    "            depth_paths = [os.path.join(subj_path, timestamps_depth[ts]) for ts in window_ts]\n",
    "\n",
    "            seq_3d = extract_3d_sequence(rgb_paths, depth_paths)\n",
    "            if seq_3d is not None:\n",
    "                SEQ_X_3D.append(seq_3d)\n",
    "                SEQ_Y_3D.append(label)\n",
    "\n",
    "SEQ_X_3D = np.array(SEQ_X_3D)\n",
    "SEQ_Y_3D = np.array(SEQ_Y_3D)\n",
    "\n",
    "print(\" RGBD 3D Sequences Extracted:\", SEQ_X_3D.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec9676-64d5-4f3a-b2a0-196465d0b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Flatten for PCA: (342 Ã— 10, 63)\n",
    "X_3D_flat = SEQ_X_3D.reshape(-1, 63)\n",
    "\n",
    "# Apply PCA\n",
    "pca_3D = PCA(n_components=10)\n",
    "X_3D_pca_flat = pca_3D.fit_transform(X_3D_flat)\n",
    "\n",
    "# Reshape back to sequence format: (342, 10, 10)\n",
    "X_seq_3D_pca = X_3D_pca_flat.reshape(SEQ_X_3D.shape[0], SEQ_X_3D.shape[1], -1)\n",
    "\n",
    "print(\" Shape after PCA (RGBD):\", X_seq_3D_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b826484-06aa-405e-a134-5b429d1633d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the RGBD data\n",
    "X_train_3D, X_test_3D, y_train_3D, y_test_3D = train_test_split(\n",
    "    X_seq_3D_pca, SEQ_Y_3D, test_size=0.2, stratify=SEQ_Y_3D, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_3D_cat = to_categorical(y_train_3D)\n",
    "y_test_3D_cat = to_categorical(y_test_3D)\n",
    "\n",
    "# Define LSTM model\n",
    "model_3D = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 10)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_3D.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train with early stopping\n",
    "history_3D = model_3D.fit(\n",
    "    X_train_3D, y_train_3D_cat,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1c4f1-9d36-470e-8637-2a1376b3bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_3D = model_3D.predict(X_test_3D)\n",
    "y_pred_3D_class = np.argmax(y_pred_3D, axis=1)\n",
    "y_true_3D = np.argmax(y_test_3D_cat, axis=1)\n",
    "\n",
    "print(\"\\nRGBD LSTM Classification Report:\\n\")\n",
    "print(classification_report(y_true_3D, y_pred_3D_class, target_names=[\"Healthy\", \"Limited\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a25ff8-9abd-4046-99dc-c35de5720712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split without PCA\n",
    "X_train_3D_raw, X_test_3D_raw, y_train_3D_raw, y_test_3D_raw = train_test_split(\n",
    "    SEQ_X_3D, SEQ_Y_3D, test_size=0.2, stratify=SEQ_Y_3D, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode\n",
    "y_train_3D_raw_cat = to_categorical(y_train_3D_raw)\n",
    "y_test_3D_raw_cat = to_categorical(y_test_3D_raw)\n",
    "\n",
    "# Define new LSTM model (adjusted for 63 input features)\n",
    "model_3D_raw = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 63)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_3D_raw.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history_3D_raw = model_3D_raw.fit(\n",
    "    X_train_3D_raw, y_train_3D_raw_cat,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4c6eb-8e0c-4bcc-8ce4-28ed14f0d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_3D_raw = model_3D_raw.predict(X_test_3D_raw)\n",
    "y_pred_3D_raw_class = np.argmax(y_pred_3D_raw, axis=1)\n",
    "y_true_3D_raw = np.argmax(y_test_3D_raw_cat, axis=1)\n",
    "\n",
    "print(\"\\nRGBD LSTM (No PCA) Classification Report:\\n\")\n",
    "print(classification_report(y_true_3D_raw, y_pred_3D_raw_class, target_names=[\"Healthy\", \"Limited\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d0187-0903-4cbb-b8bd-95818e483ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jittered_sequences_3d(sequences, labels, jitter_std=0.005, num_copies=1):\n",
    "    synthetic_X = []\n",
    "    synthetic_y = []\n",
    "\n",
    "    for _ in range(num_copies):\n",
    "        for seq, label in zip(sequences, labels):\n",
    "            noise = np.random.normal(loc=0.0, scale=jitter_std, size=seq.shape)\n",
    "            jittered = seq + noise\n",
    "            synthetic_X.append(jittered)\n",
    "            synthetic_y.append(label)\n",
    "    \n",
    "    return np.array(synthetic_X), np.array(synthetic_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471b4af-36e4-4f7e-ab52-ed52d856aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 synthetic version per real sequence\n",
    "synthetic_X_3D, synthetic_y_3D = create_jittered_sequences_3d(SEQ_X_3D, SEQ_Y_3D, jitter_std=0.005, num_copies=1)\n",
    "\n",
    "# Combine real + synthetic\n",
    "X_3D_augmented = np.concatenate([SEQ_X_3D, synthetic_X_3D], axis=0)\n",
    "y_3D_augmented = np.concatenate([SEQ_Y_3D, synthetic_y_3D], axis=0)\n",
    "\n",
    "print(\"Augmented RGBD dataset shape:\", X_3D_augmented.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d32ba0-8041-4c51-82c1-4fdd06be7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Flatten: (684 Ã— 10, 63)\n",
    "X_3D_aug_flat = X_3D_augmented.reshape(-1, 63)\n",
    "\n",
    "# Fit and apply PCA\n",
    "pca_aug = PCA(n_components=10)\n",
    "X_3D_aug_pca_flat = pca_aug.fit_transform(X_3D_aug_flat)\n",
    "\n",
    "# Reshape back: (684, 10, 10)\n",
    "X_seq_3D_aug_pca = X_3D_aug_pca_flat.reshape(X_3D_augmented.shape[0], X_3D_augmented.shape[1], -1)\n",
    "\n",
    "print(\"Shape after PCA on augmented data:\", X_seq_3D_aug_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15116ed6-b629-4188-8551-8d9345664d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train-test split\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
    "    X_seq_3D_aug_pca, y_3D_augmented, test_size=0.2, stratify=y_3D_augmented, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_aug_cat = to_categorical(y_train_aug)\n",
    "y_test_aug_cat = to_categorical(y_test_aug)\n",
    "\n",
    "# LSTM model\n",
    "model_aug = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 10)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_aug.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history_aug = model_aug.fit(\n",
    "    X_train_aug, y_train_aug_cat,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20476de-045f-47ca-b275-e27493d58196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_aug = model_aug.predict(X_test_aug)\n",
    "y_pred_aug_class = np.argmax(y_pred_aug, axis=1)\n",
    "y_true_aug = np.argmax(y_test_aug_cat, axis=1)\n",
    "\n",
    "print(\"\\nAugmented RGBD LSTM Classification Report:\\n\")\n",
    "print(classification_report(y_true_aug, y_pred_aug_class, target_names=[\"Healthy\", \"Limited\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84648239-a717-4100-938c-94443429df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\n",
    "    \"RGB + PCA\", \"RGB + RP\", \"RGBD + PCA\", \"RGBD (no PCA)\", \n",
    "    \"RGBD + PCA + Jitter\"\n",
    "]\n",
    "accuracies = [0.75, 0.64, 0.77, 0.64, 0.93]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, accuracies, color='skyblue')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Accuracy Across Models\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb11dc-c7a2-44cd-9aa1-f4f6c627f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_true_aug, y_pred_aug_class)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Healthy\", \"Limited\"], \n",
    "            yticklabels=[\"Healthy\", \"Limited\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (RGBD + Augmented)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7961890-cde4-4d4c-84df-c223eb63c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_hand_pose_3d(landmarks_3d):\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    landmarks_3d = np.array(landmarks_3d).reshape(21, 3)\n",
    "    ax.scatter(landmarks_3d[:, 0], landmarks_3d[:, 1], landmarks_3d[:, 2], color='blue')\n",
    "    ax.set_title(\"3D Hand Pose from RGBD\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_zlabel(\"z\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9998026-bab7-41e7-8eb7-56910b6a4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_temp = PCA(n_components=30)\n",
    "pca_temp.fit(X_3D_aug_flat)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(np.cumsum(pca_temp.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Variance Explained (RGBD)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62721ba8-a63a-41f2-9c1b-b5646f4c1a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
